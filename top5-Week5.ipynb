{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Pandas and Data Cleanup\n",
    "\n",
    "#### Recap from Previous Class:\n",
    "- Fetched data using Yelp API\n",
    "- Analyzed and processed the data for the Top5 App\n",
    "- Pulled duration data using Mapbox API\n",
    "- Learned about the following concepts:\n",
    "    - Pandas Data Structures\n",
    "    - DataFrame and Series attributes\n",
    "    - Selecting and Subsetting in DataFrames (.loc and .iloc)\n",
    "    - Pass by Reference vs Pass by Values\n",
    "    - Looping through DataFrame rows\n",
    "    - Method Chaining\n",
    "    - Apply method and lambda\n",
    "    - Mapping values in DataFrames\n",
    "\n",
    "#### Topics Covered in Videos:\n",
    "- Missing Values\n",
    "- Wrangling, Reshaping and Pivot Tables\n",
    "- Data Analysis: Split-Apply-Combine and Pandas GroupBy\n",
    "- Pandas str and regex\n",
    "\n",
    "---\n",
    "\n",
    "#### Our Focus for today:\n",
    "1. Understand how to deal with missing values \n",
    "    - An in-depth review of recognizing patters and handling missing values in terms of the context\n",
    "    - Use ML for imputation\n",
    "2. Learn about dummy variables and their creation\n",
    "3. Apply the concept of GroupBy-Split-Combine\n",
    "4. Learn about Pivot, Reshape and Melt\n",
    "5. Str and Regex in Python\n",
    "    - Processing Text Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/app_image.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Task 0: Fetching Yelp Data and Structuring DataFrame</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Yelp Key\n",
    "from configparser import ConfigParser\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "API_KEY = config[\"YELP\"][\"api-key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from yelp\n",
    "import requests\n",
    "import json\n",
    "\n",
    "base_url = 'https://api.yelp.com/v3'\n",
    "endpoint = '/businesses/search'\n",
    "\n",
    "# Construct the full URL for the API request\n",
    "url = f\"{base_url}{endpoint}\"\n",
    "headers = {'Authorization': API_KEY}\n",
    "params = {\n",
    "    'location': 'Denver, CO',  # Location to search for businesses\n",
    "    'limit': 50,               # Limit the number of results to 50\n",
    "    'term': 'restaurant',      # Search term to look for restaurants\n",
    "    'radius': 2000             # Search radius in meters\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Make the GET request to the Yelp API\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    # If the request is successful, print the results in a formatted JSON\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "    else:\n",
    "        # Raise an exception if the request was not successful\n",
    "        response.raise_for_status()\n",
    "except requests.exceptions.RequestException as e:\n",
    "    # Print an error message if an exception occurs during the request\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fetching businesses after pagination (set to 50 results per page)\n",
    "import pandas as pd\n",
    "restaurant_df = pd.DataFrame(result['businesses'])\n",
    "\n",
    "# Append data\n",
    "for i in range(1, 5):\n",
    "    url_params = {\n",
    "        'location': 'Denver, CO',\n",
    "        'limit': 50,\n",
    "        'term': 'restaurant',\n",
    "        'radius': 5000,\n",
    "        'offset': (i*50) # Offset for pagination\n",
    "    }\n",
    "    if i == 4:\n",
    "        url_params['limit'] = 40 # Limit the last page to 40 results\n",
    "    try:\n",
    "        # Make the GET request to the Yelp API for additional pages\n",
    "        response = requests.get(url, headers=headers, params=url_params)\n",
    "        if response.status_code == 200:\n",
    "            result = json.loads(response.content)\n",
    "            data = result['businesses']\n",
    "            new_df = pd.DataFrame(data)\n",
    "            restaurant_df = pd.concat([restaurant_df, new_df], ignore_index=True)\n",
    "        else:\n",
    "            # Raise an exception if the request was not successful\n",
    "            response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Print an error message if an exception occurs during the request\n",
    "        print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deep copy\n",
    "restaurant_df_clean = restaurant_df.copy(deep=True)\n",
    "\n",
    "# capatialize columns\n",
    "restaurant_df_clean.columns = restaurant_df_clean.columns.str.capitalize()\n",
    "\n",
    "# Subset columns using loc\n",
    "restaurant_df_clean = restaurant_df_clean.loc[:, ['Id', 'Name', 'Is_closed', 'Review_count', 'Categories', \n",
    "                                      'Rating', 'Coordinates', 'Transactions', 'Price', 'Location',\n",
    "                                      'Distance', 'Business_hours']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract display address from location\n",
    "restaurant_df_clean['Location'] = restaurant_df_clean['Location'].apply(lambda x: x['display_address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data types\n",
    "restaurant_df_clean = restaurant_df_clean.astype({\n",
    "    'Price': 'category',\n",
    "    'Name': 'string',\n",
    "    'Id':'string'\n",
    "})\n",
    "\n",
    "def alter_transactions(df):\n",
    "    for transaction in ['delivery', 'pickup', 'restaurant_reservation']:\n",
    "        df.loc[:, transaction] = 0\n",
    "        df.loc[:, transaction] = df.loc[:, 'Transactions'].apply(lambda x: 1 if transaction in x else 0)\n",
    "\n",
    "alter_transactions(restaurant_df_clean)\n",
    "\n",
    "restaurant_df_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Values\n",
    "\n",
    "Understanding the nature of missing data is crucial for choosing appropriate handling methods, such as imputation techniques or adjustments in statistical models. The interpretation can significantly impact the validity of conclusions drawn from the data.\n",
    "\n",
    "There are several ways to interpret missing values in data science, each with different implications for analysis and modeling. Here are some common interpretations:\n",
    "\n",
    "1. **Missing Completely at Random (MCAR)**:\n",
    "   - Data is missing due to entirely random factors\n",
    "   - No relationship between the missing data and other variables\n",
    "   - Least problematic for analysis. Safe to remove MCAR data as it does not introduce bias into the analysis\n",
    "   - E.g: Data missing due to technical issues or equipment malfunctions.\n",
    "\n",
    "2. **Missing at Random (MAR)**:\n",
    "   - Missing data is related to other observed variables but not to the missing variable itself\n",
    "   - Requires careful handling to avoid bias. Techniques like imputation or weighting can be used\n",
    "   - E.g. Data missing due to known characteristics of the population, such as non-response in surveys.\n",
    "\n",
    "3. **Missing Not at Random (MNAR)**:\n",
    "   - Missing data is related to the unobserved values themselves\n",
    "   - Most challenging to handle and can lead to biased results\n",
    "   - Requires specialized techniques like multiple imputation, sensitivity analysis or selection models to avoid bias.\n",
    "   - E.g. Data missing due to the nature of the variable (e.g., sensitive questions in surveys)\n",
    "\n",
    "<img src=\"./images/missing_vals.png\" width=\"400\">\n",
    "\n",
    "4. Structural missingness:\n",
    "   - Data is missing due to the inherent structure of the data collection process\n",
    "   - Example: Questions that are only asked if a previous question is answered in a certain way\n",
    "\n",
    "5. Truncated data:\n",
    "   - Values beyond a certain threshold are completely unobserved\n",
    "\n",
    "6. Intermittent missingness:\n",
    "   - Data is missing for some time points in a time series\n",
    "   - Common in longitudinal studies\n",
    "\n",
    "7. Dropout:\n",
    "   - Subjects leave a study before its completion, leading to missing data in later time points\n",
    "\n",
    "8. Informative missingness:\n",
    "   - The fact that data is missing provides information about the underlying phenomenon\n",
    "\n",
    "\n",
    "Additional Readings:\n",
    "- https://www.geeksforgeeks.org/ml-handling-missing-values/\n",
    "- https://www.freecodecamp.org/news/how-to-handle-missing-data-in-a-dataset/\n",
    "- https://www.mastersindatascience.org/learning/how-to-deal-with-missing-data/\n",
    "- https://www.linkedin.com/advice/3/what-some-common-causes-types-missing-values-datasets\n",
    "- https://www.linkedin.com/advice/0/what-best-practices-identifying-handling-missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Task 1: Handle Missing Price Values</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value count with missing values\n",
    "restaurant_df_clean.loc[:, 'Price'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_df_imputed = restaurant_df_clean.copy(deep=True)\n",
    "# fill missing values with mode\n",
    "restaurant_df_imputed.loc[:, 'Price'].fillna(restaurant_df_imputed.loc[:, 'Price'].mode()[0], inplace=False).value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try imputation with Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare the data\n",
    "X = restaurant_df_imputed.loc[:, ['Rating', 'Review_count', 'Price']]\n",
    "y = restaurant_df_imputed['Price']\n",
    "\n",
    "# Do label encoding\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.loc[X['Price'].notnull(), ['Rating', 'Review_count']]\n",
    "X_test = X.loc[X['Price'].isnull(), ['Rating', 'Review_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_price = restaurant_df_imputed['Price'].isnull()\n",
    "\n",
    "# Impute missing values\n",
    "restaurant_df_imputed.loc[missing_price, 'Price'] = \\\n",
    "le.inverse_transform(\n",
    "    rf_model.predict(\n",
    "        restaurant_df_imputed.loc[missing_price, ['Rating', 'Review_count']]\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts\n",
    "restaurant_df_imputed.loc[:, 'Price'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Dummy Variables\n",
    "\n",
    "- Representing categorical data: They allow us to include categorical variables in Machine Learning models that require numerical inputs.\n",
    "- Avoiding ordinal relationships: Dummy coding prevents implying an ordinal relationship where none exists.\n",
    "- Improved interpretability: Each dummy variable has a clear interpretation in the context of the model.\n",
    "\n",
    "#### <span style='color:blue'>Task 2: Create dummy for categories</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check categories value\n",
    "restaurant_df_imputed.loc[51, 'Categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each row, create a category list\n",
    "restaurant_df_imputed['Categories'] = restaurant_df_imputed['Categories'].apply(lambda x: [category['title'] for category in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through categories using itertuples\n",
    "category_set = dict()\n",
    "for row in restaurant_df_imputed.itertuples():\n",
    "    for category in row.Categories:\n",
    "        category_set[category] = category_set.get(category, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take top 10\n",
    "category_set = dict(sorted(category_set.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display category_set\n",
    "category_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables\n",
    "for category in category_set:\n",
    "    restaurant_df_imputed[category] = restaurant_df_imputed['Categories'].apply(lambda x: 1 if category in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_df_imputed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use pd.get_dummies for simpler ways of creating dummies for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for Price column\n",
    "pd.get_dummies(restaurant_df_imputed, columns=['Price']).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby-Split-Combine\n",
    "\n",
    "- Group: Divide your data into groups based on some common characteristic.\n",
    "- Split: Separate the data into these groups.\n",
    "- Apply: Perform an operation or calculation on each group independently.\n",
    "\n",
    "<img src=\"./images/groupby.png\" width=\"600\">\n",
    "\n",
    "\n",
    "A list of all the functions can be found in the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#grouping-operations)\n",
    "\n",
    "\n",
    "| Function          | Description                                                           |\n",
    "|-------------------|-----------------------------------------------------------------------|\n",
    "| **Aggregation Functions** |\n",
    "| `sum()`           | Compute the sum of group values.                                      |\n",
    "| `mean()`          | Compute the mean of group values.                                     |\n",
    "| `median()`        | Compute the median of group values.                                   |\n",
    "| `min()`           | Compute the minimum of group values.                                  |\n",
    "| `max()`           | Compute the maximum of group values.                                  |\n",
    "| `count()`         | Compute the count of group values.                                    |\n",
    "| `size()`          | Compute the size of each group.                                       |\n",
    "| `std()`           | Compute the standard deviation of group values.                       |\n",
    "| `var()`           | Compute the variance of group values.                                 |\n",
    "| `sem()`           | Compute the standard error of the mean of group values.               |\n",
    "| `first()`         | Compute the first value of each group.                                |\n",
    "| `last()`          | Compute the last value of each group.                                 |\n",
    "| `nth(n)`          | Compute the nth value of each group.                                  |\n",
    "| `prod()`          | Compute the product of group values.                                  |\n",
    "| `any()`           | Returns True if any element in the group is True.                     |\n",
    "| `all()`           | Returns True if all elements in the group are True.                   |\n",
    "| **Transformation Functions** |\n",
    "| `transform(func)` | Apply a function to each group and return a DataFrame with the same shape as the original. |\n",
    "| **Apply Functions** |\n",
    "| `apply(func)`     | Apply a function to each group and combine the results into a DataFrame, Series, or scalar depending on the function's output. |\n",
    "| **Descriptive Statistics** |\n",
    "| `describe()`      | Generate descriptive statistics for each group.                       |\n",
    "| `quantile(q)`     | Compute the quantile of each group.                                   |\n",
    "| `mad()`           | Compute the mean absolute deviation of group values.                  |\n",
    "| `skew()`          | Compute the skewness of group values.                                 |\n",
    "| `kurt()`          | Compute the kurtosis of group values.                                 |\n",
    "| **Other Functions** |\n",
    "| `agg(func)`       | Aggregate using one or more operations over the specified axis.       |\n",
    "| `cumsum()`        | Compute the cumulative sum of group values.                           |\n",
    "| `cumprod()`       | Compute the cumulative product of group values.                       |\n",
    "| `cummin()`        | Compute the cumulative minimum of group values.                       |\n",
    "| `cummax()`        | Compute the cumulative maximum of group values.                       |\n",
    "| `head(n)`         | Return the first `n` rows of each group.                              |\n",
    "| `tail(n)`         | Return the last `n` rows of each group.                               |\n",
    "| `ngroup()`        | Number each group from 0 to the number of groups - 1.                 |\n",
    "| `nth(n)`          | Take the nth row from each group.                                     |\n",
    "\n",
    "#### <span style='color:blue'>Task 3: Get summarized information</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_df_imputed.groupby('Price').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby price and count and average the ratings and review-count\n",
    "restaurant_df_imputed.groupby('Price').agg({'Review_count': 'mean', 'Rating': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby price and count for all restuarant categories\n",
    "restaurant_df_imputed.groupby('Price')[list(category_set.keys())].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping: Pivot and Melt\n",
    "\n",
    "**Multi-indexing**: Multi-indexing in pandas allows you to work with more complex data structures by using multiple levels of indexes (row or column labels). This is particularly useful for handling data with multiple dimensions or hierarchical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Month': ['Jan', 'Jan', 'Jan', 'Jan', 'Feb', 'Feb', 'Feb', 'Feb'],\n",
    "    'Region': ['North', 'North', 'South', 'South', 'North', 'North', 'South', 'South'],\n",
    "    'Product': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],\n",
    "    'Sales': [100, 150, 200, 250, 110, 160, 210, 260]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index(['Month', 'Region'], inplace=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pivot**: Pivoting is the process of transforming data from a long format to a wide format. In a wide format, we use unique values from a column to create new columns. In other words, spread rows into columns.\n",
    "\n",
    "<img src=\"./images/pivot.png\" width=\"600\">\n",
    "\n",
    "**Melt**: Melting is the opposite of pivoting. It transforms data from a wide format to a long format. In a long format, we have more rows and fewer columns. In other words, it gathers columns into rows.\n",
    "\n",
    "<img src=\"./images/melt.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Task 4: Create top 5 table for each category</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter where Mexican is 1 and sortby rating\n",
    "restaurant_df_imputed.loc[restaurant_df_imputed['Mexican'] == 1, ['Name', 'Review_count', 'Price', 'Rating']].sort_values('Rating', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_df_imputed.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_indices = [1, 5, 10] + list(range(15, 25))\n",
    "restaurant_top5 = restaurant_df_imputed.copy(deep=True).iloc[:, col_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_top5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame to have a single category column\n",
    "melted = restaurant_top5.melt(\n",
    "                id_vars=['Name', 'Rating', 'Distance'], \n",
    "                 value_vars= list(category_set.keys()),\n",
    "                 var_name='Category', value_name='Is_Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only rows where Is_Category is 1\n",
    "melted = melted[melted['Is_Category'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Is_Category column\n",
    "melted = melted.drop(columns=['Is_Category'])\n",
    "\n",
    "# Sort by Category, then Ratings, and Duration\n",
    "melted = melted.sort_values(by=['Category', 'Rating', 'Distance'], ascending=[True, False, True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group by Category and take top 5\n",
    "top_5 = melted.groupby('Category').head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a ranking within each category\n",
    "top_5['Rank'] = top_5.groupby('Category').cumcount() + 1\n",
    "top_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set Multi-Index\n",
    "top_5.set_index(['Category', 'Rank'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Class Exercise: https://github.com/rahimrasool/tools1_week5/blob/main/In_class_exercise.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Str and Regex\n",
    "\n",
    "Some of the most essential string methods in Pandas:\n",
    "\n",
    "1. **`str.lower()`**: Converts strings in the Series to lowercase.\n",
    "2. **`str.upper()`**: Converts strings in the Series to uppercase.\n",
    "3. **`str.title()`**: Converts strings in the Series to titlecase (each word starts with an uppercase letter).\n",
    "4. **`str.capitalize()`**: Capitalizes the first character of each string in the Series.\n",
    "5. **`str.strip()`**: Removes leading and trailing whitespace from each string in the Series.\n",
    "6. **`str.lstrip()`**: Removes leading whitespace from each string in the Series.\n",
    "7. **`str.rstrip()`**: Removes trailing whitespace from each string in the Series.\n",
    "8. **`str.replace(old, new)`**: Replaces occurrences of a substring within each string with another substring.\n",
    "9. **`str.cat(sep='')`**: Concatenates strings in the Series with a specified separator.\n",
    "10. **`str.contains(pattern)`**: Checks if strings in the Series contain a pattern.\n",
    "11. **`str.startswith(pattern)`**: Checks if strings in the Series start with a pattern.\n",
    "12. **`str.endswith(pattern)`**: Checks if strings in the Series end with a pattern.\n",
    "13. **`str.find(sub)`**: Finds the lowest index of the substring.\n",
    "14. **`str.len()`**: Computes the length of each string in the Series.\n",
    "15. **`str.split(pat=None, n=-1, expand=False)`**: Splits strings in the Series by a specified delimiter.\n",
    "16. **`str.join(sep)`**: Joins lists in the Series into a single string with a specified separator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_df_imputed.Location = restaurant_df_imputed.Location.apply(lambda x: \",\".join(x))\n",
    "restaurant_df_imputed.Location.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'1223 E 13thCO 80218 CO'.split()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_df_imputed['Zipcode'] = restaurant_df_imputed.Location.apply(lambda x: x.split()[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_df_imputed['Zipcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.search(r\"\\b\\d{5}\\b\", '1033 E 17th Ave 80218 CO').group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DateTime in Pandas\n",
    "\n",
    "- Temporal data (consisting of date and time stamps) is highly common in data analytics and can be processed to create useful features. \n",
    "- Pandas offers the ability to work with time series information in various formats. \n",
    "- This enables us to perform several operations on datetime values such as sorting, predicting or categorizing data by certain time periods.\n",
    "\n",
    "</br>\n",
    "\n",
    "- Datetime from Python’s datetime library provides the ability for manipulating dates and times. \n",
    "- We will work with datetime.datetime type which is a combination of both date and time. \n",
    "- Pandas will assign either datetime64[ns] or datetime64[ns, tz] data type to such a format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting a single scalar value\n",
    "pd.to_datetime(\"2010/11/12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dates = pd.date_range(start='2024-01-01', end='2024-01-31')\n",
    "stores = ['Littleton', 'Aurora', 'Highlands']\n",
    "products = ['Airmax', 'Airforce', 'Jordans']\n",
    "sales_data = pd.DataFrame({\n",
    "    'date': np.random.choice(dates, size=300),\n",
    "    'store': np.random.choice(stores, size=300),\n",
    "    'product': np.random.choice(products, size=300),\n",
    "    'sales': np.random.randint(10, 100, size=300)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every Timestamp has a set of time/date properties or temporal features that can be extracted.\n",
    "\n",
    "For a Series of type datetime, we can use the .dt accessor to extract these properties. This [table](https://pandas.pydata.org/docs/user_guide/timeseries.html#time-date-components) lists all the properties that can be accessed. We will use it to get the day of the week (to classify if the sale was on a weekend or a weekday). We will assign it to a new column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.loc[:,'Weekday'] = sales_data.loc[:,'date'].dt.weekday\n",
    "sales_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask and RAPIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
